import requests
from bs4 import BeautifulSoup
import time
import json

# Function to scrape links with "Who is Hiring" titles
def scrape_hiring_links(base_url, start_year, end_year):
    hiring_data = []
    next_page = f"{base_url}&next=0"
    
    while next_page:
        response = requests.get(next_page)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Find all submission titles
            links = soup.find_all("a")
            for link in links:
                title = link.get_text().strip()
                if "who is hiring" in title.lower():
                    url = "https://news.ycombinator.com/" + link["href"]
                    # Extract additional data (e.g., submission date)
                    item_page = requests.get(url)
                    if item_page.status_code == 200:
                        item_soup = BeautifulSoup(item_page.text, "html.parser")
                        date_tag = item_soup.find("span", class_="age")
                        date = date_tag.get_text().strip() if date_tag else "Unknown"
                        hiring_data.append({
                            "url": url,
                            "title": title,
                            "date": date
                        })
                        time.sleep(0.5)  # Avoid rate limiting
                    else:
                        print(f"Failed to retrieve details for {url}")
            
            # Find the next page link
            more_link = soup.find("a", text="More")
            if more_link:
                next_page = "https://news.ycombinator.com/" + more_link["href"]
            else:
                next_page = None
            
            # Optional: print progress
            print(f"Scraped {len(hiring_data)} entries so far...")
            
            # To avoid rate limiting, pause between requests
            time.sleep(1)
        else:
            print(f"Failed to retrieve page. Status code: {response.status_code}")
            break
    
    # Filter by year (2020-2024)
    filtered_data = [
        entry for entry in hiring_data if any(str(year) in entry["date"] for year in range(start_year, end_year + 1))
    ]
    
    return filtered_data

# Hacker News base URL for submissions
base_url = "https://news.ycombinator.com/submitted?id=whoishiring"

# Scrape links from 2020 to 2024
hiring_details = scrape_hiring_links(base_url, 2020, 2024)

# Save results to a JSON file
output_file = "who_is_hiring_details.json"
with open(output_file, "w") as f:
    json.dump(hiring_details, f, indent=4)

print(f"Extracted data has been saved to {output_file}")
