{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae883f0-813b-442f-8251-77f59a03a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Fetch the webpage content\n",
    "url = \"https://news.ycombinator.com/item?id=42297424\"  # Hacker News job thread URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Parse the webpage using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Extract all text\n",
    "all_text = soup.get_text(separator='\\n')  # Get all text from the page\n",
    "\n",
    "# Step 4: Define enhanced regex to match job posts and their details\n",
    "# This regex captures job posts that follow the \"Company | Location | Role | Format\" format\n",
    "job_pattern = re.compile(\n",
    "    r'^(.*? \\| .*? \\| .*?(?:Full-Time|Contract|Remote|Onsite).*? \\| .*?)$',\n",
    "    re.MULTILINE\n",
    ")\n",
    "\n",
    "# Find all matching job posts\n",
    "job_posts = job_pattern.findall(all_text)\n",
    "\n",
    "# Step 5: Collect detailed job descriptions by matching blocks after job posts\n",
    "detailed_jobs = []\n",
    "lines = all_text.split('\\n')  # Split text into lines for processing\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    if job_pattern.match(line):\n",
    "        # Found a job post, start collecting details\n",
    "        job_post = line.strip()\n",
    "        description = []\n",
    "\n",
    "        # Collect subsequent lines (descriptions) until the next job post or blank line\n",
    "        for next_line in lines[idx + 1:]:\n",
    "            if next_line.strip() == \"\" or job_pattern.match(next_line):\n",
    "                break\n",
    "            description.append(next_line.strip())\n",
    "\n",
    "        # Combine the job post and its description\n",
    "        detailed_jobs.append((job_post, \" \".join(description)))\n",
    "\n",
    "# Step 6: Display detailed job posts\n",
    "if detailed_jobs:\n",
    "    print(f\"Found {len(detailed_jobs)} detailed job posts:\\n\")\n",
    "    for i, (job, desc) in enumerate(detailed_jobs, 1):\n",
    "        print(f\"Job {i}:\\n{job}\\nDescription: {desc}\\n{'-'*40}\\n\")\n",
    "else:\n",
    "    print(\"No detailed job posts found.\")\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Fetch the webpage content\n",
    "url = \"https://news.ycombinator.com/item?id=42297424\"  # Hacker News job thread URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Parse the webpage using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Extract all text\n",
    "all_text = soup.get_text(separator='\\n')  # Get all text from the page\n",
    "\n",
    "# Step 4: Define enhanced regex to match job posts and their details\n",
    "# This regex captures job posts that follow the \"Company | Location | Role | Format\" format\n",
    "job_pattern = re.compile(\n",
    "    r'^(.*? \\| .*? \\| .*?(?:Full-Time|Contract|Remote|Onsite).*? \\| .*?)$',\n",
    "    re.MULTILINE\n",
    ")\n",
    "\n",
    "# Find all matching job posts\n",
    "job_posts = job_pattern.findall(all_text)\n",
    "\n",
    "# Step 5: Collect detailed job descriptions by matching blocks after job posts\n",
    "detailed_jobs = []\n",
    "lines = all_text.split('\\n')  # Split text into lines for processing\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    if job_pattern.match(line):\n",
    "        # Found a job post, start collecting details\n",
    "        job_post = line.strip()\n",
    "        description = []\n",
    "\n",
    "        # Collect subsequent lines (descriptions) until the next job post or blank line\n",
    "        for next_line in lines[idx + 1:]:\n",
    "            if next_line.strip() == \"\" or job_pattern.match(next_line):\n",
    "                break\n",
    "            description.append(next_line.strip())\n",
    "\n",
    "        # Combine the job post and its description\n",
    "        detailed_jobs.append((job_post, \" \".join(description)))\n",
    "\n",
    "# Step 6: Display detailed job posts\n",
    "if detailed_jobs:\n",
    "    print(f\"Found {len(detailed_jobs)} detailed job posts:\\n\")\n",
    "    for i, (job, desc) in enumerate(detailed_jobs, 1):\n",
    "        print(f\"Job {i}:\\n{job}\\nDescription: {desc}\\n{'-'*40}\\n\")\n",
    "else:\n",
    "    print(\"No detailed job posts found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
